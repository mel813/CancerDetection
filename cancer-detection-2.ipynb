{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models, Input\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nimport keras_tuner as kt\nfrom keras_tuner import RandomSearch\nimport seaborn as sns\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:03:53.048222Z","iopub.execute_input":"2025-07-24T14:03:53.048402Z","iopub.status.idle":"2025-07-24T14:04:12.632078Z","shell.execute_reply.started":"2025-07-24T14:03:53.048384Z","shell.execute_reply":"2025-07-24T14:04:12.631503Z"}},"outputs":[{"name":"stderr","text":"2025-07-24 14:03:58.486664: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753365838.759116      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753365838.833993      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Problem Statement\n\nThe goal of this project is to detect the presence of metastatic cancer in small image patches taken from large scans. The label will map to either 0 or 1 as cancer or non-cancerous.","metadata":{}},{"cell_type":"code","source":"#read in the dataset\ndata = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')\nimg_set = '/kaggle/input/histopathologic-cancer-detection/train/'\n\nprint(data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualize data format\ndata.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA)\n\nVisualize and clean the dataset. This included checking for null values, duplicates, and visualizing distribution of data such as the label(cancerous vs non-cancerous). The data set appeared clean, with no duplicates, or null values so cleaning was minimal. Pixel sizes were normalized to enhance training performance. Based on the initial EDA, the plan for analysis would be to load the data, normalize the data, and build a model to classify the image as either cancerous or non-cancerous. After that, AUC score will be used to assess model performance.","metadata":{}},{"cell_type":"code","source":"# check for null values in data set\nprint(data.isnull().sum())\n\n# check for duplciate values\nprint(f\"Duplicate IDs: {data['id'].duplicated().sum()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Visualize the distribution of the target variable\nsns.countplot(x='label', data=data, palette='Set2')\nplt.title('Cancer Label Distribution')\nplt.xlabel('Label (0 = Non-Cancerous, 1 = Cancerous)')\nplt.ylabel('Count')\nplt.xticks([0, 1], ['Non-Cancerous', 'Cancerous'])  # Optional for better labels\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Architecture\n\nI began with a compact baseline CNN model consisting of two convolutional blocks followed by a dense layer. This architecture was chosen to establish a performance benchmark while minimizing overfitting risk and training time on moderate-sized data.\n\nBaseline Model Configuration:\n\n- Optimizer: Adam\n- Learning Rate: 0.0005\n- Dropout: None\n- Batch Size: 64\n- Epochs: 10\n- Output Activation: Sigmoid (for binary classification)\n- Loss Function: Binary Crossentropy\n- Evaluation Metric: AUC (Area Under the ROC Curve)\nPerformance:\nAUC Score: 0.871 on the validation set\n\nThis initial model performed reasonably well, indicating that even a relatively shallow network could capture key features in the data.\n\nTo improve upon the baseline, I developed a second, deeper CNN architecture with more convolutional layers, additional filters, and regularization via dropout. This allowed the model to learn more complex spatial hierarchies while controlling overfitting.\n\nChanges in the Second Model:\n\n- Increased number of convolutional layers (from 2 to 4)\n- Increased filter sizes (e.g., 64 â†’ 128)\n- Added a Dropout layer (0.5) after convolutional blocks\n- Increased dense layer size to 128 units\n- Kept optimizer, learning rate, and batch size the same for fair comparison\n  \nThis model was tuned further using hyperparameter search (e.g., dropout rate, filter size, dense units), and performance was compared using AUC and ROC curves.","metadata":{}},{"cell_type":"code","source":"# Sample smaller subset for faster iteration\nsample_df = data.sample(10000, random_state=42)\n\n# Load and preprocess images\ndef load_images(df, img_set, img_size=(96, 96)):\n    images = []\n    labels = []\n    for _, row in df.iterrows():\n        img_path = os.path.join(img_set, row['id'] + '.tif')\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, img_size)\n        img = img / 255.0  # Normalize\n        images.append(img)\n        labels.append(row['label'])\n    return np.array(images), np.array(labels)\n\nX, y = load_images(sample_df, img_set)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#set-up baseline CNN model with 2 layers and fixed parameters\nmodel = models.Sequential([\n    Input(shape=(96, 96, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    \n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nearly_stop = EarlyStopping(monitor='val_auc', patience=3, mode='max', restore_best_weights=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0005),\n              loss='binary_crossentropy',\n              metrics=['accuracy', AUC(name='auc')])\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=10,\n    batch_size=64,\n    callbacks=[early_stop]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#define model pipeline for hyperparameter tuning\ndef build_model(hp):\n    model = models.Sequential()\n    model.add(layers.Input(shape=(96, 96, 3)))\n    \n    model.add(layers.Conv2D(\n        filters=hp.Choice('conv_1_filters', [32, 64]),\n        kernel_size=hp.Choice('conv_1_kernel', [3, 5]),\n        activation='relu'\n    ))\n    \n    model.add(layers.Conv2D(\n        filters=hp.Choice('conv_2_filters', [64, 128]),\n        kernel_size=3,\n        activation='relu'\n    ))\n    \n    model.add(layers.MaxPooling2D(pool_size=2))\n    model.add(layers.Dropout(hp.Float('dropout_rate', 0.3, 0.5, step=0.1)))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(\n        units=hp.Choice('dense_units', [64, 128, 256]),\n        activation='relu'\n    ))\n    model.add(layers.Dense(1, activation='sigmoid'))\n\n    model.compile(\n        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-4, 5e-4, 1e-3])),\n        loss='binary_crossentropy',\n        metrics=[AUC(name='auc')]\n    )\n    \n    return model\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for best parameters from model, commented out as it is slow\n#tuner = RandomSearch(\n#    build_model,\n#    objective='val_auc',\n#    max_trials=10,          \n#    executions_per_trial=1, \n#    directory='cancer_tune',\n#    project_name='cnn_tune'\n#)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tuner.search(X_train, y_train,\n#             epochs=10,\n#            validation_data=(X_val, y_val),\n#             batch_size=64,\n#             callbacks=[early_stop])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#best_model = tuner.get_best_models(num_models=1)[0]\n#best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n#print(f\"Best hyperparameters: {best_hps.values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#use the best parameters from the model with hyperparameter tuning to build the final model\ndef build_final_model():\n    model = models.Sequential()\n    \n    # First convolutional block\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    # Second convolutional block\n    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    # Flatten + dense layers\n    model.add(layers.Flatten())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Dense(64, activation='relu'))\n\n    # Output layer for binary classification\n    model.add(layers.Dense(1, activation='sigmoid'))\n\n    # Compile model\n    model.compile(\n        optimizer=Adam(learning_rate=0.0001),\n        loss='binary_crossentropy',\n        metrics=[AUC(name='auc')]\n    )\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results and Analysis\n\nResults of the two model architectures were compared using the ROC(based on AUC), since that is the metric for this competition. I also visualized the confusion matrix for the tuned model. \n\nThe AUC value for the baseline model was 0.871. I also applied early stopping for the baseline model based on the AUC score. The AUC value for the model with the best parameters from hyperparameter tuning was 0.880. Hyperparameter tuning was done for convolutional filters, kernel sizes, dropout rates, dense layer sizes, and learning rates. The best model had the following parameters: {'conv_1_filters': 32, 'conv_1_kernel': 3, 'conv_2_filters': 128, 'dropout_rate': 0.3, 'dense_units': 64, 'learning_rate': 0.0001}. The hyperparameter tuning code was commented out as it takes some time to run and would slow down submission time. The hyperparameters identified were used to build a final model. In future iterations, I would try the best model with different optimizers as well.","metadata":{}},{"cell_type":"code","source":"#build model with best parameters\nfinal_model = build_final_model()\n\nhistory = final_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=10,\n    batch_size=64,\n    callbacks=[early_stop]\n)\n\nfinal_model.save('best_cancer_model.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict probabilities with baseline model\ny_pred_baseline = model.predict(X_val).ravel()\nfpr_base, tpr_base, _ = roc_curve(y_val, y_pred_baseline)\nauc_base = auc(fpr_base, tpr_base)\n\n# Predict probabilities with tuned best model\ny_pred_tuned = final_model.predict(X_val).ravel()\nfpr_final, tpr_final, _ = roc_curve(y_val, y_pred_tuned)\nauc_final = auc(fpr_final, tpr_final)\n\n# Plot both ROC curves\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_base, tpr_base, linestyle='--', label=f'Baseline Model (AUC = {auc_base:.3f})')\nplt.plot(fpr_final, tpr_final, linestyle='-', label=f'Tuned Model (AUC = {auc_final:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison: Baseline vs Tuned Model')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#confusion matrix for baseline model\ny_pred_labels = (y_pred_baseline > 0.5).astype(int)\ncm = confusion_matrix(y_val, y_pred_labels)\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Cancer', 'Cancer'], yticklabels=['No Cancer', 'Cancer'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix - Baseline Model')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#confusion matrix for tuned model\ny_pred_labels = (y_pred_tuned > 0.5).astype(int)\ncm = confusion_matrix(y_val, y_pred_labels)\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['No Cancer', 'Cancer'], yticklabels=['No Cancer', 'Cancer'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix - Final Tuned Model')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Conclusion\n\nBased on the confusion matrices, the tuned model had an increase in true negatives (TN) but a decrease in true positives (TP), suggesting that it became more conservative in flagging positive cases. This trade-off may indicate a shift in the decision threshold or possible overfitting to negative examples. In future iterations, I would try balancing the data (e.g., using oversampling or class weights) to reduce bias toward the majority class and help the model better generalize.\n\nAdditionally, I would explore tuning the classification threshold to optimize for metrics such as F1-score or recall, depending on whether false negatives or false positives are more critical in this medical context. Further improvements might also include experimenting with data augmentation, more advanced architectures like EfficientNet, or using ensembling methods to improve robustness.\n\nFinally, evaluating model performance using precision-recall curves in addition to ROC-AUC may provide deeper insight, especially when dealing with class imbalance.","metadata":{}},{"cell_type":"code","source":"# Load the model\nmodel = load_model('/kaggle/working/best_cancer_model.h5')\n\n# Load sample submission\nsubmission_df = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/sample_submission.csv')\n\n# Add .tif to image ids for filename matching\nsubmission_df['id'] = submission_df['id'].apply(lambda x: x + '.tif')\n\n# Image generator for test set\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=submission_df,\n    directory='/kaggle/input/histopathologic-cancer-detection/test/',\n    x_col='id',\n    y_col=None,\n    target_size=(96, 96),\n    batch_size=64,\n    class_mode=None,\n    shuffle=False\n)\n\n# Predict\npredictions = model.predict(test_generator, verbose=1)\n\n# Assign predictions (flatten if needed)\nsubmission_df['label'] = predictions.ravel()\n\n# Remove .tif to match submission format\nsubmission_df['id'] = submission_df['id'].str.replace('.tif', '', regex=False)\n\n# Save submission\nsubmission_df.to_csv('submission.csv', index=False)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}